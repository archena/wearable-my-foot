{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling running power with the metrics provided by Stryd\n",
    "\n",
    "In this notebook, we analyse the run recordings collected using Stryd power meter, and try to model the running power using the other metrics that Stryd provides. \n",
    "\n",
    "Stryd is the state-of-the-art closed-source running power meter, that is mounted on foot, and calculates/measures power using the built-in sensors (accelerometer, gyroscope, altimeter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fitparse import FitFile\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import .fit file as pandas data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_fit(path):\n",
    "    fit = FitFile(path)\n",
    "    \n",
    "    def record_to_series(record):\n",
    "        return pd.Series({f.name: f.value for f in record.fields})\n",
    "\n",
    "    df = pd.DataFrame([record_to_series(record) for record in fit.get_messages(\"record\")]).drop([\"timestamp\", \"distance\", \"heart_rate\", \"enhanced_altitude\", \"enhanced_speed\", \"speed\", \"Form Power\"], axis=1)\n",
    "    return df\n",
    "\n",
    "df = pd.concat([\n",
    "    read_fit(\"../data/stryd/stryd-backgaden-1km.fit\"),\n",
    "    read_fit(\"../data/stryd/stryd-up-and-down.fit\"),\n",
    "    read_fit(\"../data/stryd/stryd-sport-field-circles.fit\"),\n",
    "    read_fit(\"../data/stryd/stryd-sport-field-lap.fit\"),\n",
    "    read_fit(\"../data/stryd/stryd-sport-field.fit\"),\n",
    "    read_fit(\"../data/stryd/stryd-backgaden-0.3km.fit\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Absolute altitude and distance are not useful for our purposes. However, changes of altitude and distance over time might be important features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:,\"altitude_diff\"] = df.altitude.diff()\n",
    "df.loc[:,\"distance_diff\"] = df.Distance.diff()\n",
    "df.drop([\"altitude\", \"Distance\"], axis=1, inplace=True)\n",
    "df = df.dropna()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove all the data points that have zero or close to zero power, as they bare no information for our task (under the assumption that 0 W power is measured iff there's no motion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.power > 5]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation\n",
    "\n",
    "We explore what features correlate to power -- our target variable\n",
    "\n",
    "Observations:\n",
    "* The strongest positive correlations to power can be seen for speed, followed by air power and cadence.\n",
    "* Stance time shows strong negative correlation with leg spring stiffness, followed by power, cadence, vertical oscillation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(rc={\"axes.labelsize\":18})\n",
    "sns.pairplot(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df.loc[:, [\"power\", \"cadence\", \"stance_time\", \"Speed\", \"Air Power\", \"Leg Spring Stiffness\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(\"talk\")\n",
    "display(df.describe())\n",
    "sns.heatmap(df.corr(), cmap=\"RdBu\", center=0.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling\n",
    "## Helper functions\n",
    "\n",
    "For the purpose of our experiments we perform 5-fold cross validation, to get less biased performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cv_stats(reg, X, y, name=\"\"):\n",
    "    cv = pd.DataFrame(cross_validate(reg, X, y, scoring=['r2', 'neg_mean_absolute_error']))\n",
    "    return pd.Series({\n",
    "        \"mae_mean\": cv.test_neg_mean_absolute_error.mean(),\n",
    "        \"mae_std\": cv.test_neg_mean_absolute_error.std(),\n",
    "        \"r2_mean\": cv.test_r2.mean(),\n",
    "        \"r2_std\": cv.test_r2.std(),\n",
    "    }, name=name)\n",
    "\n",
    "def get_cv_mae(reg_class, alpha, X, y):\n",
    "    reg = reg_class(alpha=alpha)\n",
    "    return get_cv_stats(reg, X, y).mae_mean\n",
    "\n",
    "def get_cv_r2(reg_class, alpha, X, y):\n",
    "    reg = reg_class(alpha=alpha)\n",
    "    return get_cv_stats(reg, X, y).r2_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "* Separate the features (`X`) and the target (`y`)\n",
    "* Some ML models benefit from features being uniformly scaled, hence Standard Scaler is applied (`X_scaled` and `y_scaled`)\n",
    "* From physical models, we know that power is not related to speed linearly, but rather to a square of speed. Therefore, we can transform our features into combinations of polynomial terms (with degree up to 2) -- `X_polynomial`\n",
    "* In addition, `X_selected` contains hand picked features. Cadence, vertical oscillation, stance time and speed are the features that can be approximated from accelerometer data, and should be relevant for power calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,1:]\n",
    "X_scaled = pd.DataFrame(StandardScaler().fit_transform(X), columns=X.columns)\n",
    "X_selected = X_scaled.loc[:,[\"cadence\", \"vertical_oscillation\", \"stance_time\", \"Speed\"]]\n",
    "\n",
    "poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_polynomial = poly_features.fit_transform(X_scaled)\n",
    "feature_names = list(poly_features.get_feature_names())\n",
    "for i, x in enumerate(X.columns):\n",
    "    feature_names = [y.replace(f\"x{i}\", x) for y in feature_names]\n",
    "X_polynomial = pd.DataFrame(X_polynomial, columns=feature_names)\n",
    "\n",
    "y = df.iloc[:,0]\n",
    "y_scaled = StandardScaler().fit_transform(y.values.reshape(-1,1)).reshape(-1,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "It's the simplest regression model, that fits `y=sum(w_i * x_i) + b` minimising sum of square difference between the predicted and the target values.\n",
    "\n",
    "Note on the performance metrics:\n",
    "* Negative mean absolute error (MAE) -- higher values (or lower absolute values) are better\n",
    "* R^2 score -- coefficient of determination, a proportion of the target variance that can be predicted from the input features. Values closer to 1.0 are better\n",
    "\n",
    "Fitting the regression with the basic set of features, gives average performance of R^2 = 0.606"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_naive_cv_summary = get_cv_stats(LinearRegression(), X_scaled, y, \"Ordinary Least Squares\")\n",
    "lr_naive_cv_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting with only the selected features makes the performance significantly worse, with R^2 of 0.385"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_manual_cv_summary = get_cv_stats(LinearRegression(), X_selected, y, \"Ordinary Least Squares (Selected)\")\n",
    "lr_manual_cv_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the polynomial features, the performance gets even worse with R^2 of 0.256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_poly_cv_summary = get_cv_stats(LinearRegression(), X_polynomial, y, \"Ordinary Least Squares (Polynomial)\")\n",
    "lr_poly_cv_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression\n",
    "Ridge regression can be used to attempt improving performance of the ordinary linear regression. In addition to minimising the square error, Ridge introduces a penalty for the size of the coefficients (L2 regularisation).\n",
    "\n",
    "As opposed to the ordinary Linear Regression, Ridge requires a parameter for weight penalty (alpha). We try a range of different alpha values, and pick one that gives the best MAE on cross-validation.\n",
    "\n",
    "The performance of Ridge with optimised alpha is slightly better than that of the original model, with R^2=0.619"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = np.arange(0.1,100.0,0.1)\n",
    "r2 = [get_cv_r2(Ridge, x, X_scaled, y) for x in alpha]\n",
    "sns.lineplot(alpha, r2)\n",
    "plt.xlabel(\"alpha\")\n",
    "plt.ylabel(\"R^2\")\n",
    "plt.title(\"Original Features\")\n",
    "plt.show()\n",
    "best_alpha = alpha[np.argmax(r2)]\n",
    "best_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_cv_summary = get_cv_stats(Ridge(best_alpha), X_scaled, y, \"Ridge\")\n",
    "ridge_cv_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting Ridge with polynomial features gives the best performance for alpha = 11.7, however R^2 is still lower than that of the plain linear model, even though the performance is significantly better if compared to the orignial model fitted with the polynomial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = np.arange(0.1,20.0,0.1)\n",
    "r2 = [get_cv_r2(Ridge, x, X_polynomial, y) for x in alpha]\n",
    "sns.lineplot(alpha, r2)\n",
    "plt.xlabel(\"alpha\")\n",
    "plt.ylabel(\"R^2\")\n",
    "plt.title(\"Polynomial Features\")\n",
    "plt.show()\n",
    "best_alpha = alpha[np.argmax(r2)]\n",
    "best_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_poly_cv_summary = get_cv_stats(Ridge(best_alpha), X_polynomial, y, \"Ridge (Polynomial)\")\n",
    "ridge_poly_cv_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regression\n",
    "Another improvement over the ordinary Linear Regression is Lasso. It employs L1 reguralisation, that penalises non-zero coefficients, effectively acting as feature selection.\n",
    "\n",
    "Same as Ridge, it requires an alpha parameter.\n",
    "\n",
    "Using the original features, we get the model, which is better than our original model, and shows R^2 of 0.638."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = np.arange(0.1,3.0,0.01)\n",
    "r2 = [get_cv_r2(Lasso, x, X_scaled, y) for x in alpha]\n",
    "sns.lineplot(alpha, r2)\n",
    "plt.xlabel(\"alpha\")\n",
    "plt.ylabel(\"R^2\")\n",
    "plt.title(\"Original Features\")\n",
    "plt.show()\n",
    "best_alpha = alpha[np.argmax(r2)]\n",
    "best_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_cv_summary = get_cv_stats(Lasso(best_alpha), X_scaled, y, \"Lasso\")\n",
    "lasso_cv_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observing the coefficients, Lasso Regression model mainly makes use of speed, leg spring stiffness and change in altitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Lasso(best_alpha).fit(X_scaled, y)\n",
    "coef = pd.Series(model.coef_, index=X_scaled.columns)\n",
    "display(\"Weights\",coef[coef.abs() > 0])\n",
    "\n",
    "display(\"Intercept\", model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the same procedure to the polynomial features and alpha = 0.30, the performance is significantly better than that of the original model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = np.arange(0.1,3.0,0.1)\n",
    "r2 = [get_cv_r2(Lasso, x, X_polynomial, y) for x in alpha]\n",
    "sns.lineplot(alpha, r2)\n",
    "plt.xlabel(\"alpha\")\n",
    "plt.ylabel(\"R^2\")\n",
    "plt.title(\"Polynomial Features\")\n",
    "plt.show()\n",
    "best_alpha = alpha[np.argmax(r2)]\n",
    "best_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_poly_cv_summary = get_cv_stats(Lasso(best_alpha), X_polynomial, y, \"Lasso (Polynomial)\")\n",
    "lasso_poly_cv_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The highest positive weights are given to speed and change in altitude. However, it is unexpected for the cadence to have a large negative weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Lasso(best_alpha).fit(X_polynomial, y)\n",
    "coef = pd.Series(model.coef_, index=X_polynomial.columns)\n",
    "display(\"Weights\",coef[coef.abs() > 0])\n",
    "\n",
    "display(\"Intercept\", model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "L1 and L2 improves performance for both feature sets. And the best performing method is Lasso with the original features.\n",
    "\n",
    "Surprisingly, feature engineering does not improve the results either. For example, even though it is expected that the power is related to the square of speed, rather than the speed itself, models does not seem to utilise it at all, bringing the performance down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([\n",
    "    lr_naive_cv_summary, lr_manual_cv_summary, lr_poly_cv_summary,\n",
    "    ridge_cv_summary, ridge_poly_cv_summary,\n",
    "    lasso_cv_summary, lasso_poly_cv_summary,\n",
    "])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
