{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fitparse import FitFile\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import .fit file as pandas data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_fit(path):\n",
    "    fit = FitFile(path)\n",
    "    \n",
    "    def record_to_series(record):\n",
    "        return pd.Series({f.name: f.value for f in record.fields})\n",
    "\n",
    "    df = pd.DataFrame([record_to_series(record) for record in fit.get_messages(\"record\")]).drop([\"timestamp\", \"position_lat\", \"position_long\", \"distance\", \"heart_rate\", \"enhanced_altitude\", \"enhanced_speed\", \"speed\", \"Form Power\"], axis=1)\n",
    "    return df\n",
    "\n",
    "df = pd.concat([\n",
    "    read_fit(\"../data/Stryd 68kg 181cm backgarden 1km.fit\"),\n",
    "    read_fit(\"../data/stryd-up-n-down.fit\"),\n",
    "    read_fit(\"../data/mellor sports field circles (stryd).fit\"),\n",
    "    read_fit(\"../data/mellor sports field lap (stryd).fit\"),\n",
    "    read_fit(\"../data/mellor sports field to home (stryd).fit\"),\n",
    "    read_fit(\"../data/Stryd 68km 181cm 0.3km backgarden run.fit\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Absolute altitude and distance are not useful for our purposes. However, changes of altitude and distance over time might be important features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:,\"altitude_diff\"] = df.altitude.diff()\n",
    "df.loc[:,\"distance_diff\"] = df.Distance.diff()\n",
    "df.drop([\"altitude\", \"Distance\"], axis=1, inplace=True)\n",
    "df = df.dropna()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove all the data points that have zero or close to zero power, as they bare no information for our task (under the assumption that 0 W power is measured iff there's no motion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.power > 5]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation\n",
    "\n",
    "We explore what features correlate to power -- our target variable\n",
    "\n",
    "Observations:\n",
    "* The strongest positive correlations to power can be seen for speed, followed by air power and cadence.\n",
    "* Stance time shows strong negative correlation with leg spring stiffness, followed by power, cadence, vertical oscillation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(rc={\"axes.labelsize\":18})\n",
    "sns.pairplot(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(\"talk\")\n",
    "display(df.describe())\n",
    "sns.heatmap(df.corr(), cmap=\"RdBu\", center=0.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling\n",
    "## Helper functions\n",
    "\n",
    "For the purpose of our experiments we perform 5-fold cross validation, to get less biased performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cv_stats(reg, X, y, name=\"\"):\n",
    "    cv = pd.DataFrame(cross_validate(reg, X, y, scoring=['r2', 'neg_mean_absolute_error']))\n",
    "    return pd.Series({\n",
    "        \"mae_mean\": cv.test_neg_mean_absolute_error.mean(),\n",
    "        \"mae_std\": cv.test_neg_mean_absolute_error.std(),\n",
    "        \"r2_mean\": cv.test_r2.mean(),\n",
    "        \"r2_std\": cv.test_r2.std(),\n",
    "    }, name=name)\n",
    "\n",
    "def get_cv_mae(reg_class, alpha, X, y):\n",
    "    reg = reg_class(alpha=alpha)\n",
    "    return get_cv_stats(reg, X, y).mae_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "* Separate the features (`X`) and the target (`y`)\n",
    "* Some ML models benefit from features being uniformly scaled, hence Standard Scaler is applied (`X_scaled` and `y_scaled`)\n",
    "* From physical models, we know that power is not related to speed linearly, but rather to a square of speed. Therefore, we can transform our features into combinations of polynomial terms (with degree up to 2) -- `X_polynomial`\n",
    "* In addition, `X_selected` contains hand picked features. Cadence, vertical oscillation, stance time and speed are the features that can be approximated from accelerometer data, and should be relevant for power calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,1:]\n",
    "X_scaled = pd.DataFrame(StandardScaler().fit_transform(X), columns=X.columns)\n",
    "X_selected = X_scaled.loc[:,[\"cadence\", \"vertical_oscillation\", \"stance_time\", \"Speed\"]]\n",
    "\n",
    "poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_polynomial = poly_features.fit_transform(X_scaled)\n",
    "feature_names = list(poly_features.get_feature_names())\n",
    "for i, x in enumerate(X.columns):\n",
    "    feature_names = [y.replace(f\"x{i}\", x) for y in feature_names]\n",
    "X_polynomial = pd.DataFrame(X_polynomial, columns=feature_names)\n",
    "\n",
    "y = df.iloc[:,0]\n",
    "y_scaled = StandardScaler().fit_transform(y.values.reshape(-1,1)).reshape(-1,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "It's the simplest regression model, that fits `y=sum(w_i * x_i) + b` minimising sum of square difference between the predicted and the target values.\n",
    "\n",
    "Note on the performance metrics:\n",
    "* Negative mean absolute error (MAE) -- higher values (or lower absolute values) are better\n",
    "* R^2 score -- coefficient of determination, a proportion of the target variance that can be predicted from the input features. Values closer to 1.0 are better\n",
    "\n",
    "Fitting the regression with the basic set of features, gives average performance of R^2 = 0.623"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_naive_cv_summary = get_cv_stats(LinearRegression(), X_scaled, y, \"Linear\")\n",
    "lr_naive_cv_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting with only the selected features makes the performance significantly worse, with R^2 of 0.428"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_manual_cv_summary = get_cv_stats(LinearRegression(), X_selected, y, \"Linear (selected)\")\n",
    "lr_manual_cv_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the polynomial features, the performance gets even worse with R^2 of 0.377"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_poly_cv_summary = get_cv_stats(LinearRegression(), X_polynomial, y, \"Linear (polynomial)\")\n",
    "lr_poly_cv_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression\n",
    "Ridge regression can be used to attempt improving performance of the ordinary linear regression. In addition to minimising the square error, Ridge introduces a penalty for the size of the coefficients (L2 regularisation).\n",
    "\n",
    "As opposed to the ordinary Linear Regression, Ridge requires a parameter for weight penalty (alpha). We try a range of different alpha values, and pick one that gives the best MAE on cross-validation.\n",
    "\n",
    "However, we can see that any alpha greater than 0, decreases the performance of a model fitted on the original set of features.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = np.arange(0.1,20.0,0.1)\n",
    "mae = [get_cv_mae(Ridge, x, X_scaled, y) for x in alpha]\n",
    "sns.lineplot(alpha, mae)\n",
    "plt.xlabel(\"alpha\")\n",
    "plt.ylabel(\"Negative MAE\")\n",
    "plt.title(\"Original Features\")\n",
    "plt.show()\n",
    "best_alpha = alpha[np.argmax(mae)]\n",
    "best_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_cv_summary = get_cv_stats(Ridge(best_alpha), X_scaled, y, \"Ridge\")\n",
    "ridge_cv_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting Ridge with polynomial features gives the best performance for alpha = 13.0, however R^2 is still lower than that of the plain linear model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = np.arange(0.1,20.0,0.1)\n",
    "mae = [get_cv_mae(Ridge, x, X_polynomial, y) for x in alpha]\n",
    "sns.lineplot(alpha, mae)\n",
    "plt.xlabel(\"alpha\")\n",
    "plt.ylabel(\"Negative MAE\")\n",
    "plt.title(\"Polynomial Features\")\n",
    "plt.show()\n",
    "best_alpha = alpha[np.argmax(mae)]\n",
    "best_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_poly_cv_summary = get_cv_stats(Ridge(best_alpha), X_polynomial, y, \"Ridge (Polynomial)\")\n",
    "ridge_poly_cv_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regression\n",
    "Another improvement over the ordinary Linear Regression is Lasso. It employs L1 reguralisation, that penalises non-zero coefficients, effectively acting as feature selection.\n",
    "\n",
    "Same as Ridge, it requires an alpha parameter.\n",
    "\n",
    "Using the original features, we again see no improvement in using large regularisation coefficients, and R^2 of 0.806, which is marginally better than our original model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = np.arange(0.1,3.0,0.01)\n",
    "mae = [get_cv_mae(Lasso, x, X_scaled, y) for x in alpha]\n",
    "sns.lineplot(alpha, mae)\n",
    "plt.xlabel(\"alpha\")\n",
    "plt.ylabel(\"Negative MAE\")\n",
    "plt.title(\"Original Features\")\n",
    "plt.show()\n",
    "best_alpha = alpha[np.argmax(mae)]\n",
    "best_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_cv_summary = get_cv_stats(Lasso(best_alpha), X_scaled, y, \"Lasso\")\n",
    "lasso_cv_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observing the coefficients, Lasso Regression model mainly makes use of speed and change in altitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Lasso(best_alpha).fit(X_scaled, y)\n",
    "coef = pd.Series(model.coef_, index=X_scaled.columns)\n",
    "display(\"Weights\",coef[coef.abs() > 0])\n",
    "\n",
    "display(\"Intercept\", model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the same procedure to the polynomial features and alpha = 0.25, the performance is slightly worse than that of the original model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = np.arange(0.1,3.0,0.01)\n",
    "mae = [get_cv_mae(Lasso, x, X_polynomial, y) for x in alpha]\n",
    "sns.lineplot(alpha, mae)\n",
    "plt.xlabel(\"alpha\")\n",
    "plt.ylabel(\"Negative MAE\")\n",
    "plt.title(\"Polynomial Features\")\n",
    "plt.show()\n",
    "best_alpha = alpha[np.argmax(mae)]\n",
    "best_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_poly_cv_summary = get_cv_stats(Lasso(best_alpha), X_polynomial, y, \"Lasso (Polynomial)\")\n",
    "lasso_poly_cv_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The highest positive weights are given to speed and change in altitude. However, it is unexpected for the cadence to have a large negative weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Lasso(best_alpha).fit(X_polynomial, y)\n",
    "coef = pd.Series(model.coef_, index=X_polynomial.columns)\n",
    "display(\"Weights\",coef[coef.abs() > 0])\n",
    "\n",
    "display(\"Intercept\", model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "L1 and L2 regularisation does not seem to improve performance of the ordinary linear model, bringing only a marginal increase in R^2.\n",
    "\n",
    "Surprisingly, feature engineering does not improve the results either. For example, even though it is expected that the power is related to the square of speed, rather than the speed itself, models does not seem to utilise it at all, bringing the performance down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([\n",
    "    lr_naive_cv_summary, lr_manual_cv_summary, lr_poly_cv_summary,\n",
    "    ridge_cv_summary, ridge_poly_cv_summary,\n",
    "    lasso_cv_summary, lasso_poly_cv_summary,\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_est = X_scaled.Speed*27.855256 + X_scaled.altitude_diff*27.855256 + 242.73024236037935\n",
    "deviation = pd.Series(y.values - y_est.values)\n",
    "display(deviation.describe())\n",
    "display(\"Mean Absolute Error\", deviation.abs().mean())\n",
    "sns.distplot(y.values - y_est.values, kde=False)\n",
    "plt.xlabel(\"Deviation from Stryd Power measurement\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
